{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1ac95a",
   "metadata": {},
   "source": [
    "# LangChain RAG Lab: Resume Q&A with Robo1! ðŸ“„ðŸ¤–\n",
    "\n",
    "Welcome to the LangChain RAG (Retrieval-Augmented Generation) Lab! In this notebook, you'll build a chatbot that answers interview questions using your resume. Robo1 is here to helpâ€”he loves finding answers in big piles of documents!\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"langchain/images/robo1.png\" alt=\"Robo1 - RAG Expert\" width=\"120\" /></td>\n",
    "<td style=\"vertical-align:top; padding-left:20px;\">\n",
    "<b>Robo1 says:</b><br>\n",
    "<i>\"I can search your resume faster than you can say 'curriculum vitae'!\"</i><br>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "Let's get started and see how RAG makes chatbots smarter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c393d81",
   "metadata": {},
   "source": [
    "# What is RAG (Retrieval-Augmented Generation)?\n",
    "RAG combines the power of search (retrieval) with language models (generation). Instead of guessing, your chatbot can look up answers in documentsâ€”like your resume!\n",
    "\n",
    "Robo1 loves RAG because it means less guessing and more knowing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c7201",
   "metadata": {},
   "source": [
    "# Import Libraries and Setup\n",
    "Let's import the necessary libraries and set up our environment for RAG-powered resume Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42cb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai\n",
    "# pip install python-dotenv\n",
    "# pip install faiss-cpu\n",
    "# pip install langchain_community\n",
    "# pip install pypdf\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API key from environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331ea21",
   "metadata": {},
   "source": [
    "# Load and Chunk Your Resume\n",
    "Let's load your resume PDF and split it into smaller chunks so Robo1 can search it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19025a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"my_resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split resume into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d14705",
   "metadata": {},
   "source": [
    "# Create a Vector Store and Retriever\n",
    "We'll turn your resume chunks into searchable vectors so Robo1 can find the best answers fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6105f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d84830",
   "metadata": {},
   "source": [
    "# Build the RAG Chatbot Chain\n",
    "Now we'll connect everything together so Robo1 can answer your interview questions using your resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f894957",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are answering interview questions based on the following resume context.\n",
    "If the answer is not in the context, say \\\"I don't know based on my resume.\\\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer as if you are the candidate:\n",
    "\"\"\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PromptTemplate.from_template(prompt_template)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a499dc",
   "metadata": {},
   "source": [
    "# Chat with Your Resume!\n",
    "Ask interview questions and see how Robo1 answers using your resume. If he can't find the answer, he'll let you know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf279012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: {'query': 'What are your technical skills?', 'result': '\\nSome of my technical skills include working with AWS, Apache Spark, Kubernetes, and Airflow. I have experience in handling and creating robust big data pipelines and ensuring best and secure practices. I have also successfully migrated teams from on-premise to cloud using Kubernetes. Additionally, I have a strong background in advanced querying, data federation, and integration techniques. I am also knowledgeable in AI technologies, particularly in GenAI fundamentals and practical applications with tools like LangChain and Hugging Face. I have also integrated GenAI into real-world data workflows.'}\n"
     ]
    }
   ],
   "source": [
    "# Ask your interview question here\n",
    "question = \"What are your technical skills?\"\n",
    "answer = qa_chain.invoke({\"query\": question})\n",
    "print(\"Bot:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eed6e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"langchain/images/robo1.png\" alt=\"Robo1 - RAG Expert\" width=\"120\" /></td>\n",
    "<td style=\"vertical-align:top; padding-left:20px;\">\n",
    "<b>Robo1 says:</b><br>\n",
    "<i>\"Congratulations! You just built a RAG-powered chatbot. Now I can ace any interviewâ€”unless they ask about my favorite pizza topping!\"</i><br>\n",
    "<i>With RAG, your AI can find answers in documents faster than ever. ðŸ“„ðŸ¤–</i>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "*Thanks for completing the LangChain RAG Lab!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
