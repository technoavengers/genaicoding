{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1ac95a",
   "metadata": {},
   "source": [
    "# LangChain RAG Lab: Resume Q&A with Robo1! ðŸ“„ðŸ¤–\n",
    "\n",
    "Welcome to the LangChain RAG (Retrieval-Augmented Generation) Lab! In this notebook, you'll build a chatbot that answers interview questions using your resume. Robo1 is here to helpâ€”he loves finding answers in big piles of documents!\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/robo1.png\" alt=\"Robo1 - RAG Expert\" width=\"120\" /></td>\n",
    "<td style=\"vertical-align:top; padding-left:20px;\">\n",
    "<b>Robo1 says:</b><br>\n",
    "<i>\"I can search your resume faster than you can say 'curriculum vitae'!\"</i><br>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "Let's get started and see how RAG makes chatbots smarter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c393d81",
   "metadata": {},
   "source": [
    "# What is RAG (Retrieval-Augmented Generation)?\n",
    "RAG combines the power of search (retrieval) with language models (generation). Instead of guessing, your chatbot can look up answers in documentsâ€”like your resume!\n",
    "\n",
    "Robo1 loves RAG because it means less guessing and more knowing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c7201",
   "metadata": {},
   "source": [
    "# Import Libraries and Setup\n",
    "Let's import the necessary libraries and set up our environment for RAG-powered resume Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42cb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API key from environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331ea21",
   "metadata": {},
   "source": [
    "# Load and Chunk Your Resume\n",
    "Let's load your resume PDF and split it into smaller chunks so Robo1 can search it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19025a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"my_resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split resume into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d14705",
   "metadata": {},
   "source": [
    "### Create a Vector Store and Retriever\n",
    "We'll turn your resume chunks into searchable vectors so Robo1 can find the best answers fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dc01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the FAISS vector store\n",
    "print(\"Number of documents in FAISS vectorstore:\", len(vectorstore.docstore._dict))\n",
    "print(\"Document IDs:\", list(vectorstore.docstore._dict.keys())[:5])  # Show first 5 IDs\n",
    "\n",
    "\n",
    "# Optionally, show a sample document stored in FAISS\n",
    "sample_id = list(vectorstore.docstore._dict.keys())[0]\n",
    "print(\"Sample document content:\\n\", vectorstore.docstore._dict[sample_id].page_content)\n",
    "\n",
    "# Show vector representation details\n",
    "print(\"FAISS index shape:\", vectorstore.index.ntotal, \"vectors,\", vectorstore.index.d, \"dimensions\")\n",
    "sample_vector = vectorstore.index.reconstruct(0)\n",
    "print(\"Sample vector (first 10 values):\", sample_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d84830",
   "metadata": {},
   "source": [
    "### Build the RAG Chatbot Chain\n",
    "Now we'll connect everything together so Robo1 can answer your interview questions using your resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f894957",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are answering interview questions based on the following resume context.\n",
    "If the answer is not in the context, say \\\"I don't know based on my resume.\\\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer as if you are the candidate:\n",
    "\"\"\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PromptTemplate.from_template(prompt_template)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a499dc",
   "metadata": {},
   "source": [
    "# Chat with Your Resume!\n",
    "Ask interview questions and see how Robo1 answers using your resume. If he can't find the answer, he'll let you know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf279012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: {'query': 'What are my technical skills?', 'result': '\\nAnswer: Based on your resume, you have experience with AWS, Apache Spark, Kubernetes, and Airflow. You have also worked with advanced querying, data federation, and integration techniques, as well as GenAI and AI technologies such as LangChain and Hugging Face. Additionally, you have experience with migrating teams to the cloud on Kubernetes and ensuring best and secure practices.'}\n"
     ]
    }
   ],
   "source": [
    "# Ask your interview question here\n",
    "question = \"What are my technical skills?\"\n",
    "answer = qa_chain.invoke({\"query\": question})\n",
    "print(\"Bot:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eed6e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/robo1.png\" alt=\"Robo1 - RAG Expert\" width=\"120\" /></td>\n",
    "<td style=\"vertical-align:top; padding-left:20px;\">\n",
    "<b>Robo1 says:</b><br>\n",
    "<i>\"Congratulations! You just built a RAG-powered chatbot. Now I can ace any interviewâ€”unless they ask about my favorite pizza topping!\"</i><br>\n",
    "<i>With RAG, your AI can find answers in documents faster than ever. ðŸ“„ðŸ¤–</i>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "*Thanks for completing the LangChain RAG Lab!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
